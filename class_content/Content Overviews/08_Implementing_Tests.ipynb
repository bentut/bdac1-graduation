{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Estimates, Hypothesis Tests and Experiments\n",
    "*Module: Experimental Design (Sprint 2 of 2)*\n",
    "\n",
    "*Experiments and the scientific method are at the heart of how we “know” what we know when it comes to data analysis. But how does it translate to the different situations we encounter in practice and what are some common pitfalls to be aware of?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Data Journalist| Data Engineer | Statistical Modeler| Business Analyst |\n",
    "|:----------------:|:----:|:------------------:|:----:|\n",
    "|I need to understand how **causation is established** in scientific studies so that I can interpret studies and focus my analyses|I need to be able to **implement experiment-driven algorithms** such as (A/B testing and Epsilon Greedy) so that I can provide a testing capability|I need to understand how to **isolate factors and design appropriate experiments** so that I can answer a wide range of research questions|I need to identify opportunities to test and **optimize with techniques such as A/B Testing and Epsilong Greedy** so that my organization can continuously improve|\n",
    "|I need to understand **how population characteristics are inferred** from their samples so I can draw accurate conclusions about third party research as well as my own analysis|I need to understand the computing and analytical performance **tradeoffs between different levels of sampling** so that I can optimize for different objectives|I need to understand the **kinds of statistical hypotheses I can make as well as the tests they apply to** so that I can answer a variety of research questions|I need to understand how to **construct a testable hypothesis** about the populations represented by my business data so that I can drive strategic decisions about novel scenarios|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Process Big Picture\n",
    "![Curriculum Summary](../curriculum_summary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests are powerful in any kind of learning\n",
    "- How do have we used tests so far, intentionally or not?\n",
    "- How does that apply to analysis of data?\n",
    "- Do tests and experimental design matter if we are only analyzing pre-collected data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Key Questions\n",
    "- How do we estimate characteristics of the real world using data\n",
    "- How do we express the uncertainty in our estimates?\n",
    "- What are the different ways our estimate can be wrong?\n",
    "- How do we choose our estimates?\n",
    "- What kinds of models can we create from data?\n",
    "- How do we create models from the results of our experiments?\n",
    "\n",
    "## Key Concepts and Definitions\n",
    "- effect size\n",
    "- percentile\n",
    "- quantile\n",
    "- model\n",
    "- analytic distribution\n",
    "- empirical distribution\n",
    "- standard normal distribution\n",
    "\n",
    "- confidence interval\n",
    "- standard error\n",
    "- p-value\n",
    "- null hypothesis\n",
    "- alternative hypothesis\n",
    "\n",
    "- estimation\n",
    "- estimator\n",
    "- mean squared error\n",
    "- maximum likelood estimator\n",
    "\n",
    "- false positive, false negative\n",
    "- type1 / type 2 error\n",
    "- internal / external validity\n",
    "- threats to validity\n",
    "- experimental designs\n",
    "- quasi-experimental designs\n",
    "- natural experiment\n",
    "\n",
    "\n",
    "## Themes of this Sprint\n",
    "- Estimates\n",
    "- Uncertainty\n",
    "- Hypothesis Tests\n",
    "- Validity\n",
    "- Tests / Experiments / Knowledge\n",
    "- Causality and Relationships\n",
    "- Sources of Error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Design Video Resources and Courses\n",
    "\n",
    "https://www.coursera.org/learn/real-life-data-science/lecture/Getan/experimental-design-and-observational-analysis\n",
    "\n",
    "https://www.coursera.org/learn/data-scientists-tools/lecture/NUYrv/experimental-design\n",
    "\n",
    "https://www.youtube.com/watch?v=vSXOJnGNtM4\n",
    "\n",
    "https://www.coursera.org/learn/real-life-data-science/lecture/LU8XW/a-b-testing\n",
    "\n",
    "https://www.udacity.com/course/ab-testing--ud257\n",
    "\n",
    "\n",
    "## Hypothesis Testing in Real Life\n",
    "https://www.cnn.com/2018/03/11/health/prescription-opioid-payments-eprise/index.html\n",
    "\n",
    "http://www.sigmazone.com/Clemens_Bonds_HypothesisTest.htm\n",
    "\n",
    "https://www.quora.com/What-are-some-examples-of-how-hypothesis-testing-can-be-applied-in-everyday-life\n",
    "\n",
    "## Comprehensive Link with Experimental Design Definitions\n",
    "http://www.statisticshowto.com/experimental-design/\n",
    "\n",
    "## ThinkStats Book for Statistical Concepts, Related Probability, and Hypothesis Testing\n",
    "https://github.com/devleague/BigDataAnalyst_Library/blob/master/books/thinkstats2.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Think Stats\n",
    "\n",
    "### Effect size\n",
    "> An effect size is a summary statistic intended to describe (wait for it) the size of an effect. For example, to describe the difference between two groups, one obvious choice is the difference in the means.\n",
    "\n",
    "### Percentile\n",
    "> If you have taken a standardized test, you probably got your results in the form of a raw score and a percentile rank. In this context, the percentile rank is the fraction of people who scored lower than you (or the same). So if you are “in the 90th percentile,” you did as well as or better than 90% of the people who took the exam.\n",
    "\n",
    "> *percentile rank*\n",
    "The percentage of values in a distribution that are less than or equal to a given value.\n",
    "\n",
    "> *percentile*\n",
    "The value associated with a given percentile rank.\n",
    "\n",
    "### Quantile\n",
    ">A sequence of values that correspond to equally spaced percentile ranks; for ex‐ ample, the quartiles of a distribution are the 25th, 50th and 75th percentiles.\n",
    "\n",
    "### Model / Analytic Distribution / Empirical Distribution\n",
    ">The distributions we have used so far are called empirical distributions because they are based on empirical observations, which are necessarily finite samples.\n",
    "\n",
    ">The alternative is an analytic distribution, which is characterized by a CDF that is a mathematical function. Analytic distributions can be used to model empirical distributions. In this context, a model is a simplification that leaves out unneeded details. This chapter presents common analytic distributions and uses them to model data from a variety of sources.\n",
    "\n",
    "### Standard normal distribution\n",
    ">The normal distribution, also called Gaussian, is commonly used because it describes many phenomena, at least approximately. It turns out that there is a good reason for its ubiquity, which we will get to in “Central Limit Theorem” on page 186.\n",
    "\n",
    ">The normal distribution is characterized by two parameters: the mean, μ, and standard deviation σ. The normal distribution with μ = 0 and σ = 1 is called the standard normal distribution. Its CDF is defined by an integral that does not have a closed form solution, but there are algorithms that evaluate it efficiently. One of them is provided by SciPy: scipy.stats.norm is an object that represents a normal distribution; it provides a method, cdf, that evaluates the standard normal CDF:\n",
    "\n",
    "\n",
    "> *empirical distribution*\n",
    "The distribution of values in a sample.\n",
    "\n",
    "> *analytic distribution*\n",
    "A distribution whose CDF is an analytic function.\n",
    "\n",
    "> *model*\n",
    "A useful simplification. Analytic distributions are often good models of more com‐ plex empirical distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Estimation\n",
    "> The process of inferring the parameters of a distribution from a sample.\n",
    "\n",
    "### Estimator\n",
    "> A statistic used to estimate a parameter.\n",
    "\n",
    "### Mean squared error (MSE)\n",
    "> A measure of estimation error.\n",
    "\n",
    "### Root mean squared error (RMSE)\n",
    "> The square root of MSE, a more meaningful representation of typical error mag‐ nitude.\n",
    "\n",
    "### Maximum likelihood estimator (MLE)\n",
    "> An estimator that computes the point estimate most likely to be correct.\n",
    "\n",
    "### Sampling Error\n",
    "> Variation in the estimate caused by random selection is called sampling error.\n",
    "\n",
    "> Error in an estimate due to the limited size of the sample and variation due to chance.\n",
    "\n",
    "### Confidence Interval / Standard Error\n",
    "\n",
    "> *standard error* The RMSE of an estimate, which quantifies variability due to sampling error (but not other sources of error).\n",
    "\n",
    ">*confidence interval*\n",
    "An interval that represents the expected range of an estimator if an experiment is repeated many times.\n",
    "\n",
    ">There are two common ways to summarize the sampling distribution:\n",
    "\n",
    ">**Standard error (SE)**\n",
    "\n",
    ">A measure of how far we expect the estimate to be off, on average. For each simulated experiment, we compute the error, x ̄ - μ, and then compute the root mean squared error (RMSE). In this example, it is roughly 2.5 kg.\n",
    "\n",
    ">**Confidence interval (CI)**\n",
    "\n",
    ">A range that includes a given fraction of the sampling distribution. For example, the 90% confidence interval is the range from the 5th to the 95th percentile. In this example, the 90% CI is (86, 94) kg.\n",
    "\n",
    "> Standard errors and confidence intervals are the source of much confusion:\n",
    "> - People often confuse standard error and standard deviation. Remember that stan‐ dard deviation describes variability in a measured quantity; in this example, the standard deviation of gorilla weight is 7.5 kg. Standard error describes variability in an estimate. In this example, the standard error of the mean, based on a sample of 9 measurements, is 2.5 kg.\n",
    "> - One way to remember the difference is that, as sample size increases, standard error gets smaller; standard deviation does not.\n",
    "> - People often think that there is a 90% probability that the actual parameter, μ, falls in the 90% confidence interval. Sadly, that is not true. If you want to make a claim like that, you have to use Bayesian methods (see my book, Think Bayes). \n",
    "> The sampling distribution answers a different question: it gives you a sense of how reliable an estimate is by telling you how much it would vary if you ran the experiment again.\n",
    "\n",
    ">It is important to remember that confidence intervals and standard errors only quantify sampling error; that is, error due to measuring only part of the population. The sampling distribution does not account for other sources of error, notably sampling bias and measurement error, which are the topics of the next section.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing\n",
    "\n",
    "> The fundamental question we want to address is whether the effects we see in a sample are likely to appear in the larger population. For example, in the NSFG sample we see a difference in mean pregnancy length for first babies and others. We would like to know if that effect reflects a real difference for women in the U.S., or if it might appear in the sample by chance.\n",
    "\n",
    "> There are several ways we could formulate this question, including Fisher null hypoth‐ esis testing, Neyman-Pearson decision theory, and Bayesian inference.1 What I present here is a subset of all three that makes up most of what people use in practice, which I will call classical hypothesis testing.\n",
    "\n",
    "> The goal of classical hypothesis testing is to answer the question, “Given a sample and an apparent effect, what is the probability of seeing such an effect by chance?” Here’s how we answer that question:\n",
    "\n",
    "### P Value, Null Hypothesis, Alternative Hypothesis\n",
    "\n",
    "> - The first step is to quantify the size of the apparent effect by choosing a **test statistic**. In the NSFG example, the apparent effect is a difference in pregnancy length between first babies and others, so a natural choice for the test statistic is the dif‐ ference in means between the two groups.\n",
    "> - The second step is to define a **null hypothesis**, which is a model of the system based on the assumption that the apparent effect is not real. In the NSFG example the null hypothesis is that there is no difference between first babies and others; that is, that pregnancy lengths for both groups have the same distribution.\n",
    "> - The third step is to compute a **p-value**, which is the probability of seeing the apparent effect if the null hypothesis is true. In the NSFG example, we would compute the actual difference in means, then compute the probability of seeing a difference as big, or bigger, under the null hypothesis.\n",
    "> - The last step is to interpret the result. If the p-value is low, the effect is said to be statistically significant, which means that it is unlikely to have occurred by chance. In that case we infer that the effect is more likely to appear in the larger population.\n",
    "\n",
    "> The logic of this process is similar to a proof by contradiction. To prove a mathematical statement, A, you assume temporarily that A is false. If that assumption leads to a con‐ tradiction, you conclude that A must actually be true.\n",
    "\n",
    "> Similarly, to test a hypothesis like, “This effect is real,” we assume, temporarily, that it is not. That’s the null hypothesis. Based on that assumption, we compute the probability of the apparent effect. That’s the p-value. If the p-value is low, we conclude that the null hypothesis is unlikely to be true.\n",
    "\n",
    "\n",
    ">*hypothesis testing*\n",
    "The process of determining whether an apparent effect is statistically significant.\n",
    "\n",
    ">*test statistic*\n",
    "A statistic used to quantify an effect size.\n",
    "\n",
    ">*null hypothesis*\n",
    "A model of a system based on the assumption that an apparent effect is due to chance.\n",
    "\n",
    ">*p-value*\n",
    "The probability that an effect could occur by chance.\n",
    "\n",
    ">*statistically significant*\n",
    "An effect is statistically significant if it is unlikely to occur by chance.\n",
    "- p-value\n",
    "- null hypothesis\n",
    "- alternative hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  false positive, false negative\n",
    "### type1 / type 2 error\n",
    "\n",
    "> false positive\n",
    "The conclusion that an effect is real when it is not.\n",
    "\n",
    "> false negative\n",
    "The conclusion that an effect is due to chance when it is not.\n",
    "\n",
    "> in classical hypothesis testing, an effect is considered statistically significant if the p- value is below some threshold, commonly 5%. This procedure raises two questions:\n",
    ">- • If the effect is actually due to chance, what is the probability that we will wrongly consider it significant? This probability is the false positive rate.\n",
    ">- • If the effect is real, what is the chance that the hypothesis test will fail? This prob‐ ability is the false negative rate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experimental design and analysis\n",
    "\n",
    "### internal / external validity\n",
    "> **Internal validity** is the degree to which we can appropriately conclude that the changes in X caused the changes in Y.\n",
    "\n",
    ">Internal validity is the ability to make causal conclusions. The huge advantage of randomized experiments over observational studies, is that causal conclusions are a natural outcome of the former, but dif- ficult or impossible to justify in the latter.\n",
    "\n",
    "> **External validity** is synonymous with generalizability.\n",
    "External validity (generalizability) relates to the breadth of the pop- ulation we have sampled and how well we can justify extending our results to an even broader population.\n",
    "\n",
    "### threats to validity\n",
    "\n",
    "\n",
    "### Experimental designs / Quasi Experimental Designs / Natural Experiments\n",
    "See: http://www.statisticshowto.com/experimental-design/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related / Correlation / Causation\n",
    ">If variables X and Y (e.g., the number of televisions (X) in various countries and the infant mortality rate (Y) of those countries) are found to be associated, then there are three basic possibilities. \n",
    "- First X could be causing Y (televisions lead to more health awareness, which leads to better prenatal care) \n",
    "- or Y could be causing X (high infant mortality leads to attraction of funds from richer countries, which leads to more televisions) \n",
    "- or unknown factor Z could be causing both X and Y (higher wealth in a country leads to more televisions and more prenatal care clinics). \n",
    "\n",
    ">It is worth memorizing these three cases, because they should always be considered when association is found in an observational study as opposed to a randomized experiment. (It is also possible that X and Y are related in more complicated ways including in large networks of variables with feedback loops.)\n",
    "\n",
    ">Causation (“X causes Y”) can be logically claimed if X and Y are associated, and X precedes Y, and no plausible alternative explanations can be found, par- ticularly those of the form “X just happens to vary along with some real cause of changes in Y” (called confounding)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Design\n",
    "http://www.statisticshowto.com/experimental-design/\n",
    "    \n",
    "> Experimental design is a way to carefully plan experiments in advance so that your results are both objective and valid. The terms “Experimental Design” and “Design of Experiments” are used interchangeably and mean the same thing. However, the medical and social sciences tend to use the term “Experimental Design” while engineering, industrial and computer sciences favor the term “Design of experiments.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirmatoratory vs Exploratory Research \n",
    "https://en.wikipedia.org/wiki/Research_design\n",
    "\n",
    ">Confirmatory research tests a priori hypotheses — outcome predictions that are made before the measurement phase begins. Such a priori hypotheses are usually derived from a theory or the results of previous studies. The advantage of confirmatory research is that the result is more meaningful, in the sense that it is much harder to claim that a certain result is generalizable beyond the data set. The reason for this is that in confirmatory research, one ideally strives to reduce the probability of falsely reporting a coincidental result as meaningful. This probability is known as α-level or the probability of a type I error.\n",
    "\n",
    ">Exploratory research on the other hand seeks to generate a posteriori hypotheses by examining a data-set and looking for potential relations between variables. It is also possible to have an idea about a relation between variables but to lack knowledge of the direction and strength of the relation. If the researcher does not have any specific hypotheses beforehand, the study is exploratory with respect to the variables in question (although it might be confirmatory for others). The advantage of exploratory research is that it is easier to make new discoveries due to the less stringent methodological restrictions. Here, the researcher does not want to miss a potentially interesting relation and therefore aims to minimize the probability of rejecting a real effect or relation; this probability is sometimes referred to as β and the associated error is of type II. In other words, if the researcher simply wants to see whether some measured variables could be related, he would want to increase the chances of finding a significant result by lowering the threshold of what is deemed to be significant.\n",
    "\n",
    ">Sometimes, a researcher may conduct exploratory research but report it as if it had been confirmatory ('Hypothesizing After the Results are Known', HARKing—see Hypotheses suggested by the data); this is a questionable research practice bordering on fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Experimental Design and Modeling relationships\n",
    "\n",
    ">Most formal (confirmatory) statistical analyses are based on models. Statis- tical models are ideal, mathematical representations of observable characteristics. Models are best divided into two components. **The structural component of the model (or structural model) specifies the relationships between explana- tory variables and the mean (or other key feature) of the outcome variables**. The **“random” or “error” component of the model (or error model) characterizes the deviations of the individual observations from the mean**. (Here, “error” does not indicate “mistake”.) The two model components are also called **“signal” and “noise”** respectively. \n",
    "\n",
    ">Statisticians realize that no mathematical models are perfect representations of the real world, but some are close enough to reality to be useful. A full description of a model should include all assumptions being made because statistical inference is impossible without assumptions, and sufficient deviation of reality from the assumptions will invalidate any statistical inferences.\n",
    "\n",
    ">A slightly different point of view says that models describe how the distribution of the outcome varies with changes in the explanatory variables.\n",
    "\n",
    " \n",
    "> **Statistical models have both a structural component and a random component which describe means and the pattern of deviation from the mean, respectively.**\n",
    "\n",
    "\n",
    "## Construct Validity\n",
    "> Construct validity is a characteristic of devised measurements that describes how well the measurement can stand in for the scientific concepts or “constructs” that are the real targets of scientific learning and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal Validity (concept for Next Sprint)\n",
    "http://www.indiana.edu/~educy520/sec5982/week_9/520in_ex_validity.pdf\n",
    "\n",
    ">Why is Internal Validity Important?\n",
    "We often conduct research in order to determine\n",
    "cause-and-effect relationships.\n",
    "■ Can we conclude that changes in the independent\n",
    "variable caused the observed changes in the\n",
    "dependent variable?\n",
    "■ Is the evidence for such a conclusion good or poor?\n",
    "■ If a study shows a high degree of internal validity then\n",
    "we can conclude we have strong evidence of\n",
    "causality.\n",
    "■ If a study has low internal validity, then we must\n",
    "conclude we have little or no evidence of causality.\n",
    "\n",
    "\n",
    "# Necessary Conditions for Causality\n",
    ">Three conditions that are necessary to claim that\n",
    "variable A causes changes in variable B:\n",
    "• Relationship condition: Variable A and variable B\n",
    "must be related.\n",
    "• Temporal Antecedence condition: Proper time order\n",
    "must be established.\n",
    "• Lack of Alternative Explanation Condition:\n",
    "Relationship between variable A and variable B\n",
    "must not be attributable to a confounding,\n",
    "extraneous variable.\n",
    "\n",
    "\n",
    ">Threats to internal validity compromise our confidence\n",
    "in saying that a relationship exists between the\n",
    "independent and dependent variables.\n",
    "\n",
    ">Threats to external validity compromise our\n",
    "confidence in stating whether the study’s results are\n",
    "applicable to other groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
